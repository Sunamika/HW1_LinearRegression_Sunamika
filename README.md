# HW1_LinearRegression_Sunamika

# HW1_LinearRegression: Closed-Form vs Gradient Descent 

 
Name: Sunamika Karki 

Student_ID: 700788005 

Course: Machine Learning 

 

# Overview 

This assignment shows the linear regression using two methods such as closed-form and gradient descent. Whereas, closed-form method is used to calculate intercept and slope of synthetic data in one step and gradient descent method is used to adjust parameters which can minimize the mean squared error (MSE). Therefore, its main aim is to compare both methods and visualize its result by including regression lines and the loss curves. 



# Description 

- Here a dataset with 200 sample is generated in which x input is randomly choosen between 0 and 5 and the output y is also computed using formula y = 3 + 4 * x + noise_value where noise_value is random Gaussian noise which is used to make synthetic data more realistic one. 

- After this, I used both methods and compared them so that it can fit in a straight lines. 

- Additionally, gradient descent loss curves, data points, regression lines is plotted.

  

# Result shows 

- Almost same slope and intercept given by two methods 

- In iterations it decrease gradient descent loss as it shows converging 

- Data fits very well in regression lines.

  

 
 # How does this run 

- Open this code in visual studio or Jupyter. Then, all the cells from top to bottom should be run. It shows all the printed slopes, intercept, plots, regression lines, loss curves.

  
 

# Result 

- Here, both methods have same solutions where intercept = 3.1069 and slope = 3.9124. As gradient descent loss decreases over iteration and both regression line it fits the data very well. Therefore, it confirms that these two methods produces consistent results. 

 
